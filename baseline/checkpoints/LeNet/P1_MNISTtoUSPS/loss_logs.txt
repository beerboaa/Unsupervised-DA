Begin training
Epoch:1, training loss:1.9522156715393066, test loss:1.8166667222976685, test acc:0.5461111111111111
Epoch:2, training loss:0.8554725646972656, test loss:0.9037838578224182, test acc:0.6877777777777778
Epoch:3, training loss:0.7972525954246521, test loss:0.9003217220306396, test acc:0.6922222222222222
Begin training
Epoch:1, training loss:2.300215005874634, test loss:2.299243450164795, test acc:0.19055555555555556
Epoch:2, training loss:2.286745548248291, test loss:2.2873339653015137, test acc:0.37722222222222224
Epoch:3, training loss:2.264944553375244, test loss:2.2620773315429688, test acc:0.5266666666666666
Epoch:4, training loss:2.2269983291625977, test loss:2.211534261703491, test acc:0.615
Epoch:5, training loss:2.1446757316589355, test loss:2.141213893890381, test acc:0.6316666666666667
Epoch:6, training loss:2.0597524642944336, test loss:2.0274806022644043, test acc:0.6577777777777778
Epoch:7, training loss:1.9031271934509277, test loss:1.9028362035751343, test acc:0.6605555555555556
Epoch:8, training loss:1.7936255931854248, test loss:1.6971839666366577, test acc:0.6994444444444444
Epoch:9, training loss:1.6114754676818848, test loss:1.5626552104949951, test acc:0.7044444444444444
Epoch:10, training loss:1.4450856447219849, test loss:1.4029594659805298, test acc:0.7416666666666667
Epoch:11, training loss:1.2863456010818481, test loss:1.2396888732910156, test acc:0.7594444444444445
Epoch:12, training loss:1.154571294784546, test loss:1.0627740621566772, test acc:0.7672222222222222
Epoch:13, training loss:1.008750081062317, test loss:0.9838895797729492, test acc:0.7616666666666667
Epoch:14, training loss:0.909919798374176, test loss:0.9412873983383179, test acc:0.7727777777777778
Epoch:15, training loss:0.8024765253067017, test loss:0.8233053684234619, test acc:0.7655555555555555
Epoch:16, training loss:0.715315580368042, test loss:0.8498377799987793, test acc:0.7666666666666667
Epoch:17, training loss:0.7977124452590942, test loss:0.7133477926254272, test acc:0.7788888888888889
Epoch:18, training loss:0.7245658040046692, test loss:0.7649266719818115, test acc:0.76
Epoch:19, training loss:0.7025777101516724, test loss:0.7308928966522217, test acc:0.7722222222222223
Epoch:20, training loss:0.6238356232643127, test loss:0.6732050180435181, test acc:0.7883333333333333
Epoch:21, training loss:0.5808530449867249, test loss:0.6885790824890137, test acc:0.78
Epoch:22, training loss:0.6458558440208435, test loss:0.6651657223701477, test acc:0.7911111111111111
Epoch:23, training loss:0.526434600353241, test loss:0.6657574772834778, test acc:0.7683333333333333
Epoch:24, training loss:0.6411988735198975, test loss:0.6052910685539246, test acc:0.7983333333333333
Epoch:25, training loss:0.5815232992172241, test loss:0.7005525827407837, test acc:0.7905555555555556
Epoch:26, training loss:0.6445871591567993, test loss:0.5942881107330322, test acc:0.7944444444444444
Epoch:27, training loss:0.6591377854347229, test loss:0.7201947569847107, test acc:0.79
Epoch:28, training loss:0.6222338080406189, test loss:0.6909176707267761, test acc:0.7805555555555556
Epoch:29, training loss:0.5695515275001526, test loss:0.5657302141189575, test acc:0.8033333333333333
Epoch:30, training loss:0.6024147272109985, test loss:0.6411778330802917, test acc:0.7944444444444444
Epoch:31, training loss:0.40448346734046936, test loss:0.6786484718322754, test acc:0.7827777777777778
Epoch:32, training loss:0.4181850254535675, test loss:0.671208918094635, test acc:0.7938888888888889
Epoch:33, training loss:0.4821874499320984, test loss:0.5799546837806702, test acc:0.8
Epoch:34, training loss:0.62513267993927, test loss:0.6753699779510498, test acc:0.7855555555555556
Epoch:35, training loss:0.5058857202529907, test loss:0.6261546611785889, test acc:0.7855555555555556
Epoch:36, training loss:0.47790366411209106, test loss:0.7177402973175049, test acc:0.79
Epoch:37, training loss:0.5233782529830933, test loss:0.6214020848274231, test acc:0.7927777777777778
Epoch:38, training loss:0.5143879652023315, test loss:0.6653997898101807, test acc:0.7883333333333333
Epoch:39, training loss:0.42432600259780884, test loss:0.6133965253829956, test acc:0.8055555555555556
Epoch:40, training loss:0.529778778553009, test loss:0.6408132314682007, test acc:0.7972222222222223
Epoch:41, training loss:0.4300438165664673, test loss:0.5978004932403564, test acc:0.8005555555555556
Epoch:42, training loss:0.4523758292198181, test loss:0.5757520198822021, test acc:0.8205555555555556
Epoch:43, training loss:0.5477266311645508, test loss:0.6080591082572937, test acc:0.79
Epoch:44, training loss:0.5325609445571899, test loss:0.6732169985771179, test acc:0.7961111111111111
Epoch:45, training loss:0.4216003119945526, test loss:0.5822321176528931, test acc:0.8138888888888889
Epoch:46, training loss:0.5076203942298889, test loss:0.7748173475265503, test acc:0.8077777777777778
Epoch:47, training loss:0.4435524642467499, test loss:0.5720634460449219, test acc:0.8027777777777778
Epoch:48, training loss:0.49831846356391907, test loss:0.643505871295929, test acc:0.8083333333333333
Epoch:49, training loss:0.5616087913513184, test loss:0.6331996917724609, test acc:0.805
Epoch:50, training loss:0.48871999979019165, test loss:0.5948532223701477, test acc:0.815
Epoch:51, training loss:0.4679624140262604, test loss:0.7625954747200012, test acc:0.8022222222222222
Epoch:52, training loss:0.5955000519752502, test loss:0.6359254717826843, test acc:0.7994444444444444
Epoch:53, training loss:0.28724783658981323, test loss:0.6787086725234985, test acc:0.8138888888888889
Epoch:54, training loss:0.398823082447052, test loss:0.6614881753921509, test acc:0.81
Epoch:55, training loss:0.42559564113616943, test loss:0.722335934638977, test acc:0.7988888888888889
Epoch:56, training loss:0.38167306780815125, test loss:0.5987435579299927, test acc:0.8166666666666667
Epoch:57, training loss:0.29984477162361145, test loss:0.6853412985801697, test acc:0.8061111111111111
Epoch:58, training loss:0.3781372904777527, test loss:0.6099565625190735, test acc:0.8222222222222222
Epoch:59, training loss:0.36688897013664246, test loss:0.5790004134178162, test acc:0.815
Epoch:60, training loss:0.32912299036979675, test loss:0.5309410691261292, test acc:0.8216666666666667
Epoch:61, training loss:0.47473853826522827, test loss:0.5538009405136108, test acc:0.82
Epoch:62, training loss:0.4429608881473541, test loss:0.6104915738105774, test acc:0.8144444444444444
Epoch:63, training loss:0.4356004297733307, test loss:0.5950703024864197, test acc:0.81
Epoch:64, training loss:0.35485365986824036, test loss:0.6173732280731201, test acc:0.8111111111111111
Epoch:65, training loss:0.37478917837142944, test loss:0.5870090126991272, test acc:0.8155555555555556
Epoch:66, training loss:0.4093674123287201, test loss:0.590092658996582, test acc:0.8155555555555556
Epoch:67, training loss:0.3670870363712311, test loss:0.6587594151496887, test acc:0.81
Epoch:68, training loss:0.4048752188682556, test loss:0.7777391076087952, test acc:0.8066666666666666
Epoch:69, training loss:0.46422189474105835, test loss:0.6566001772880554, test acc:0.8127777777777778
Epoch:70, training loss:0.31326910853385925, test loss:0.575980544090271, test acc:0.8222222222222222
Epoch:71, training loss:0.3573145866394043, test loss:0.7769370079040527, test acc:0.8094444444444444
Epoch:72, training loss:0.33815181255340576, test loss:0.7597221732139587, test acc:0.8172222222222222
Epoch:73, training loss:0.36627843976020813, test loss:0.6458383798599243, test acc:0.8066666666666666
Epoch:74, training loss:0.4030362367630005, test loss:0.5955435037612915, test acc:0.8072222222222222
Epoch:75, training loss:0.3531450033187866, test loss:0.5902736783027649, test acc:0.8211111111111111
Epoch:76, training loss:0.4943966269493103, test loss:0.6453700661659241, test acc:0.7977777777777778
Epoch:77, training loss:0.3561544418334961, test loss:0.8119666576385498, test acc:0.8072222222222222
Epoch:78, training loss:0.3052046000957489, test loss:0.5501316785812378, test acc:0.8188888888888889
Epoch:79, training loss:0.3857293128967285, test loss:0.6671121716499329, test acc:0.7983333333333333
Epoch:80, training loss:0.3580854535102844, test loss:0.580640435218811, test acc:0.8044444444444444
Epoch:81, training loss:0.3010313808917999, test loss:0.6268967390060425, test acc:0.8127777777777778
Epoch:82, training loss:0.4569384753704071, test loss:0.5716394186019897, test acc:0.81
Epoch:83, training loss:0.3191646337509155, test loss:0.7125333547592163, test acc:0.8072222222222222
Epoch:84, training loss:0.3756842613220215, test loss:0.6180676221847534, test acc:0.81
Epoch:85, training loss:0.38073480129241943, test loss:0.6372696161270142, test acc:0.8211111111111111
Epoch:86, training loss:0.32263660430908203, test loss:0.6375858187675476, test acc:0.8211111111111111
Epoch:87, training loss:0.3079635500907898, test loss:0.5344799757003784, test acc:0.825
Epoch:88, training loss:0.335541307926178, test loss:0.5963014960289001, test acc:0.8083333333333333
Epoch:89, training loss:0.306137353181839, test loss:0.5777790546417236, test acc:0.8127777777777778
Epoch:90, training loss:0.34882161021232605, test loss:0.6296756267547607, test acc:0.8177777777777778
Epoch:91, training loss:0.27950337529182434, test loss:0.5989243984222412, test acc:0.8177777777777778
Epoch:92, training loss:0.23630711436271667, test loss:0.6279968619346619, test acc:0.8105555555555556
Epoch:93, training loss:0.35323968529701233, test loss:0.5561227202415466, test acc:0.8188888888888889
Epoch:94, training loss:0.28581956028938293, test loss:0.5928627252578735, test acc:0.8177777777777778
Epoch:95, training loss:0.32456251978874207, test loss:0.5916422605514526, test acc:0.8116666666666666
Epoch:96, training loss:0.22933651506900787, test loss:0.5804765224456787, test acc:0.8211111111111111
Epoch:97, training loss:0.23682130873203278, test loss:0.6188435554504395, test acc:0.815
Epoch:98, training loss:0.26842620968818665, test loss:0.7034784555435181, test acc:0.8116666666666666
Epoch:99, training loss:0.287960410118103, test loss:0.7057551145553589, test acc:0.8133333333333334
Epoch:100, training loss:0.27403104305267334, test loss:0.6676535606384277, test acc:0.8161111111111111
Epoch:101, training loss:0.3255804181098938, test loss:0.6714135408401489, test acc:0.82
Epoch:102, training loss:0.2855517864227295, test loss:0.6225049495697021, test acc:0.815
Epoch:103, training loss:0.3125263452529907, test loss:0.6268864274024963, test acc:0.8027777777777778
Epoch:104, training loss:0.2756464183330536, test loss:0.6817147135734558, test acc:0.8111111111111111
Epoch:105, training loss:0.33711594343185425, test loss:0.6301197409629822, test acc:0.8044444444444444
Epoch:106, training loss:0.28206804394721985, test loss:0.5593007802963257, test acc:0.8216666666666667
Epoch:107, training loss:0.31044819951057434, test loss:0.8537348508834839, test acc:0.8122222222222222
Epoch:108, training loss:0.361240029335022, test loss:0.6961137652397156, test acc:0.8127777777777778
Epoch:109, training loss:0.3085559010505676, test loss:0.7810706496238708, test acc:0.815
Epoch:110, training loss:0.25287026166915894, test loss:0.624288022518158, test acc:0.8172222222222222
Epoch:111, training loss:0.2760832607746124, test loss:0.6788762807846069, test acc:0.8033333333333333
Epoch:112, training loss:0.2662620544433594, test loss:0.6501677632331848, test acc:0.8155555555555556
Epoch:113, training loss:0.35336652398109436, test loss:0.6178184151649475, test acc:0.8122222222222222
Epoch:114, training loss:0.2800125181674957, test loss:0.660108208656311, test acc:0.8022222222222222
Epoch:115, training loss:0.25903719663619995, test loss:0.5846863389015198, test acc:0.8172222222222222
Epoch:116, training loss:0.3129999339580536, test loss:0.6204321980476379, test acc:0.8144444444444444
Epoch:117, training loss:0.24831849336624146, test loss:0.6962818503379822, test acc:0.8194444444444444
Epoch:118, training loss:0.30144235491752625, test loss:0.7343543767929077, test acc:0.8083333333333333
Epoch:119, training loss:0.23554475605487823, test loss:0.7071006298065186, test acc:0.8144444444444444
Epoch:120, training loss:0.30525317788124084, test loss:0.6011269688606262, test acc:0.8111111111111111
Epoch:121, training loss:0.24048733711242676, test loss:0.6652514934539795, test acc:0.815
Epoch:122, training loss:0.24560242891311646, test loss:0.6353501677513123, test acc:0.8066666666666666
Epoch:123, training loss:0.23301781713962555, test loss:0.6971980929374695, test acc:0.8194444444444444
Epoch:124, training loss:0.27381423115730286, test loss:0.6007788181304932, test acc:0.8122222222222222
Epoch:125, training loss:0.3109167814254761, test loss:0.8298027515411377, test acc:0.8122222222222222
Epoch:126, training loss:0.30077996850013733, test loss:0.7448233962059021, test acc:0.8094444444444444
Epoch:127, training loss:0.34454935789108276, test loss:0.7581475377082825, test acc:0.795
Epoch:128, training loss:0.3077346384525299, test loss:0.651481568813324, test acc:0.7983333333333333
Epoch:129, training loss:0.30268725752830505, test loss:0.6161159873008728, test acc:0.81
Epoch:130, training loss:0.2682395279407501, test loss:0.6586486101150513, test acc:0.8088888888888889
Epoch:131, training loss:0.26034867763519287, test loss:0.6289440393447876, test acc:0.8122222222222222
Epoch:132, training loss:0.24722930788993835, test loss:0.7205651998519897, test acc:0.8066666666666666
Epoch:133, training loss:0.3252665102481842, test loss:0.6577197909355164, test acc:0.8094444444444444
Epoch:134, training loss:0.34601908922195435, test loss:0.9664772748947144, test acc:0.805
Epoch:135, training loss:0.21798951923847198, test loss:0.6982199549674988, test acc:0.8044444444444444
Epoch:136, training loss:0.2590409517288208, test loss:0.6849378943443298, test acc:0.8044444444444444
Epoch:137, training loss:0.23560437560081482, test loss:0.6518033146858215, test acc:0.8172222222222222
Epoch:138, training loss:0.2996464967727661, test loss:0.679978609085083, test acc:0.8072222222222222
Epoch:139, training loss:0.22707827389240265, test loss:0.7240182757377625, test acc:0.8166666666666667
Epoch:140, training loss:0.18715451657772064, test loss:0.6580771803855896, test acc:0.805
Epoch:141, training loss:0.2052391618490219, test loss:0.8381334543228149, test acc:0.7955555555555556
Epoch:142, training loss:0.293672114610672, test loss:0.9259290099143982, test acc:0.8011111111111111
Epoch:143, training loss:0.2620985507965088, test loss:0.6771923303604126, test acc:0.7983333333333333
Epoch:144, training loss:0.3109338879585266, test loss:0.6924802660942078, test acc:0.81
Epoch:145, training loss:0.17855820059776306, test loss:0.755763828754425, test acc:0.8061111111111111
Epoch:146, training loss:0.20108747482299805, test loss:0.6617348194122314, test acc:0.8011111111111111
Epoch:147, training loss:0.2621631324291229, test loss:0.6310043334960938, test acc:0.8077777777777778
Epoch:148, training loss:0.25222349166870117, test loss:0.7032000422477722, test acc:0.8083333333333333
Epoch:149, training loss:0.23245607316493988, test loss:0.7015950083732605, test acc:0.7972222222222223
Epoch:150, training loss:0.24936024844646454, test loss:0.6831432580947876, test acc:0.8061111111111111
