Begin training
Epoch:1, training loss:2.1844429969787598, test loss:2.243342399597168, test acc:0.2106
Epoch:2, training loss:1.684006929397583, test loss:1.8552817106246948, test acc:0.4151
Epoch:3, training loss:1.101894736289978, test loss:1.344886064529419, test acc:0.607
Epoch:4, training loss:0.9522883892059326, test loss:0.9960881471633911, test acc:0.7067
Epoch:5, training loss:0.6011187434196472, test loss:0.9065489172935486, test acc:0.6934
Epoch:6, training loss:0.48598235845565796, test loss:0.804395854473114, test acc:0.7326
Epoch:7, training loss:0.5231203436851501, test loss:0.7409176230430603, test acc:0.7544
Epoch:8, training loss:0.29415902495384216, test loss:0.7192065119743347, test acc:0.7698
Epoch:9, training loss:0.32758408784866333, test loss:0.7192296385765076, test acc:0.7646
Epoch:10, training loss:0.4652692377567291, test loss:0.7238146662712097, test acc:0.7644
Epoch:11, training loss:0.3209104537963867, test loss:0.6811403632164001, test acc:0.7835
Epoch:12, training loss:0.27893325686454773, test loss:0.6192994117736816, test acc:0.8006
Epoch:13, training loss:0.304576575756073, test loss:0.6278982162475586, test acc:0.7977
Epoch:14, training loss:0.25678199529647827, test loss:0.5488329529762268, test acc:0.8275
Epoch:15, training loss:0.29674863815307617, test loss:0.5582987666130066, test acc:0.8164
Epoch:16, training loss:0.2295100837945938, test loss:0.5722461938858032, test acc:0.8103
Epoch:17, training loss:0.25127652287483215, test loss:0.5201750993728638, test acc:0.8392
Epoch:18, training loss:0.2038881927728653, test loss:0.5106528401374817, test acc:0.837
Epoch:19, training loss:0.2549034059047699, test loss:0.5082793235778809, test acc:0.8396
Epoch:20, training loss:0.24196851253509521, test loss:0.48642101883888245, test acc:0.8381
Epoch:21, training loss:0.2512175738811493, test loss:0.5142009258270264, test acc:0.8298
Epoch:22, training loss:0.36360153555870056, test loss:0.4752388894557953, test acc:0.8449
Epoch:23, training loss:0.20571205019950867, test loss:0.513340175151825, test acc:0.833
Epoch:24, training loss:0.29534509778022766, test loss:0.5444850325584412, test acc:0.8177
Epoch:25, training loss:0.08912507444620132, test loss:0.4521900713443756, test acc:0.8591
Epoch:26, training loss:0.18244890868663788, test loss:0.44022828340530396, test acc:0.8536
Epoch:27, training loss:0.08989289402961731, test loss:0.4371086061000824, test acc:0.8566
Epoch:28, training loss:0.07749725878238678, test loss:0.4487132728099823, test acc:0.8507
Epoch:29, training loss:0.18641535937786102, test loss:0.41103729605674744, test acc:0.8651
Epoch:30, training loss:0.23212158679962158, test loss:0.4527958333492279, test acc:0.8579
Epoch:31, training loss:0.2070881575345993, test loss:0.4661690294742584, test acc:0.8493
Epoch:32, training loss:0.13568075001239777, test loss:0.4157726466655731, test acc:0.8632
Epoch:33, training loss:0.10854598134756088, test loss:0.39967402815818787, test acc:0.8731
Epoch:34, training loss:0.10791460424661636, test loss:0.45703521370887756, test acc:0.8514
Epoch:35, training loss:0.17708048224449158, test loss:0.43571797013282776, test acc:0.8656
Epoch:36, training loss:0.1915416717529297, test loss:0.4265455901622772, test acc:0.8656
Epoch:37, training loss:0.09696196764707565, test loss:0.3895857036113739, test acc:0.8761
Epoch:38, training loss:0.13212180137634277, test loss:0.35715997219085693, test acc:0.8832
Epoch:39, training loss:0.13054481148719788, test loss:0.3535497784614563, test acc:0.8912
Epoch:40, training loss:0.11197119951248169, test loss:0.38374635577201843, test acc:0.8764
Epoch:41, training loss:0.11428215354681015, test loss:0.3652823567390442, test acc:0.8836
Epoch:42, training loss:0.048952557146549225, test loss:0.3700668513774872, test acc:0.8889
Epoch:43, training loss:0.08277888596057892, test loss:0.3661440312862396, test acc:0.8839
Epoch:44, training loss:0.07171756774187088, test loss:0.3610391914844513, test acc:0.8863
Epoch:45, training loss:0.0757358968257904, test loss:0.395978182554245, test acc:0.8724
Epoch:46, training loss:0.16139116883277893, test loss:0.34157684445381165, test acc:0.8933
Epoch:47, training loss:0.040513474494218826, test loss:0.3734064996242523, test acc:0.8847
Epoch:48, training loss:0.10743802040815353, test loss:0.3958664834499359, test acc:0.8753
Epoch:49, training loss:0.1559431403875351, test loss:0.3894657790660858, test acc:0.8801
Epoch:50, training loss:0.226590096950531, test loss:0.38361212611198425, test acc:0.8807
Epoch:51, training loss:0.09122674912214279, test loss:0.37695661187171936, test acc:0.8842
Epoch:52, training loss:0.11382120102643967, test loss:0.3683723509311676, test acc:0.8853
Epoch:53, training loss:0.0884765237569809, test loss:0.3925471901893616, test acc:0.8887
Epoch:54, training loss:0.051702048629522324, test loss:0.41213545203208923, test acc:0.8779
Epoch:55, training loss:0.07608996331691742, test loss:0.4106556475162506, test acc:0.8765
Epoch:56, training loss:0.0645483136177063, test loss:0.43813443183898926, test acc:0.866
Epoch:57, training loss:0.061663709580898285, test loss:0.3495332598686218, test acc:0.8931
Epoch:58, training loss:0.03303784132003784, test loss:0.40773043036460876, test acc:0.8827
Epoch:59, training loss:0.14465704560279846, test loss:0.42771339416503906, test acc:0.8735
Epoch:60, training loss:0.12375391274690628, test loss:0.3940574526786804, test acc:0.8793
Epoch:61, training loss:0.11815618723630905, test loss:0.48090729117393494, test acc:0.8629
Epoch:62, training loss:0.11789415031671524, test loss:0.3972388505935669, test acc:0.8838
Epoch:63, training loss:0.0403963103890419, test loss:0.4025889039039612, test acc:0.8872
Epoch:64, training loss:0.04868358373641968, test loss:0.4061151444911957, test acc:0.8799
Epoch:65, training loss:0.09140913933515549, test loss:0.4017349183559418, test acc:0.8844
Epoch:66, training loss:0.11432234197854996, test loss:0.3774453401565552, test acc:0.8883
Epoch:67, training loss:0.030976517125964165, test loss:0.3781752288341522, test acc:0.8903
Epoch:68, training loss:0.05936295539140701, test loss:0.41002461314201355, test acc:0.8792
Epoch:69, training loss:0.06787557154893875, test loss:0.44985437393188477, test acc:0.8787
Epoch:70, training loss:0.0690692737698555, test loss:0.3771963119506836, test acc:0.8891
Epoch:71, training loss:0.0753222405910492, test loss:0.35349521040916443, test acc:0.8986
Epoch:72, training loss:0.05078176409006119, test loss:0.34409481287002563, test acc:0.9004
Epoch:73, training loss:0.06674914062023163, test loss:0.400649756193161, test acc:0.8855
Epoch:74, training loss:0.10506777465343475, test loss:0.4520606994628906, test acc:0.877
Epoch:75, training loss:0.05907129868865013, test loss:0.3887648284435272, test acc:0.8906
Epoch:76, training loss:0.0746389701962471, test loss:0.4721086621284485, test acc:0.8706
Epoch:77, training loss:0.06780973076820374, test loss:0.41697826981544495, test acc:0.8835
Epoch:78, training loss:0.03178136423230171, test loss:0.4810332953929901, test acc:0.8693
Epoch:79, training loss:0.026996146887540817, test loss:0.3980332911014557, test acc:0.8933
Epoch:80, training loss:0.03434097394347191, test loss:0.4732714295387268, test acc:0.8743
Epoch:81, training loss:0.03908950090408325, test loss:0.36874228715896606, test acc:0.8997
Epoch:82, training loss:0.06661047041416168, test loss:0.37682485580444336, test acc:0.9001
Epoch:83, training loss:0.0290362611413002, test loss:0.39334818720817566, test acc:0.8946
Epoch:84, training loss:0.030721036717295647, test loss:0.3688156306743622, test acc:0.9011
Epoch:85, training loss:0.028558867052197456, test loss:0.5019785761833191, test acc:0.8776
Epoch:86, training loss:0.052161864936351776, test loss:0.4098597466945648, test acc:0.8885
Epoch:87, training loss:0.06968946009874344, test loss:0.41615423560142517, test acc:0.8881
Epoch:88, training loss:0.035348184406757355, test loss:0.43179184198379517, test acc:0.8883
Epoch:89, training loss:0.021163608878850937, test loss:0.366028368473053, test acc:0.9007
Epoch:90, training loss:0.021722733974456787, test loss:0.41766998171806335, test acc:0.891
Epoch:91, training loss:0.022019080817699432, test loss:0.45820197463035583, test acc:0.882
Epoch:92, training loss:0.08675375580787659, test loss:0.390035480260849, test acc:0.9001
Epoch:93, training loss:0.059877511113882065, test loss:0.43925365805625916, test acc:0.888
Epoch:94, training loss:0.046545352786779404, test loss:0.42394372820854187, test acc:0.8925
Epoch:95, training loss:0.05936359614133835, test loss:0.41299349069595337, test acc:0.893
Epoch:96, training loss:0.037376388907432556, test loss:0.4579702317714691, test acc:0.8844
Epoch:97, training loss:0.0414661280810833, test loss:0.46910911798477173, test acc:0.8827
Epoch:98, training loss:0.013173851184546947, test loss:0.41881152987480164, test acc:0.8984
Epoch:99, training loss:0.019652362912893295, test loss:0.4666266441345215, test acc:0.884
Epoch:100, training loss:0.016938339918851852, test loss:0.45903339982032776, test acc:0.8843
Epoch:101, training loss:0.039751287549734116, test loss:0.41611409187316895, test acc:0.8941
Epoch:102, training loss:0.03182804584503174, test loss:0.4718290865421295, test acc:0.8857
Epoch:103, training loss:0.022611714899539948, test loss:0.5741212964057922, test acc:0.8669
Epoch:104, training loss:0.030450649559497833, test loss:0.4854895770549774, test acc:0.8834
Epoch:105, training loss:0.01900522969663143, test loss:0.4861208498477936, test acc:0.8827
Epoch:106, training loss:0.036879293620586395, test loss:0.4439759850502014, test acc:0.8917
Epoch:107, training loss:0.03657418489456177, test loss:0.4543401300907135, test acc:0.8948
Epoch:108, training loss:0.06931627541780472, test loss:0.5390892028808594, test acc:0.8757
Epoch:109, training loss:0.02759438380599022, test loss:0.5529086589813232, test acc:0.8713
Epoch:110, training loss:0.03779226914048195, test loss:0.4856841266155243, test acc:0.8866
Epoch:111, training loss:0.018029699102044106, test loss:0.46032053232192993, test acc:0.892
Epoch:112, training loss:0.041917093098163605, test loss:0.5255523920059204, test acc:0.8837
Epoch:113, training loss:0.04131080582737923, test loss:0.5796281695365906, test acc:0.869
Epoch:114, training loss:0.014070146717131138, test loss:0.511871337890625, test acc:0.8852
Epoch:115, training loss:0.024405021220445633, test loss:0.48503461480140686, test acc:0.8894
Epoch:116, training loss:0.01661558821797371, test loss:0.4810812473297119, test acc:0.8881
Epoch:117, training loss:0.07246533036231995, test loss:0.5464301109313965, test acc:0.8814
Epoch:118, training loss:0.011081197299063206, test loss:0.504684329032898, test acc:0.8863
Epoch:119, training loss:0.03997821733355522, test loss:0.516129195690155, test acc:0.883
Epoch:120, training loss:0.03476601094007492, test loss:0.5851630568504333, test acc:0.8743
Epoch:121, training loss:0.01913609355688095, test loss:0.4988522231578827, test acc:0.8911
Epoch:122, training loss:0.028418928384780884, test loss:0.5225695967674255, test acc:0.8858
Epoch:123, training loss:0.02699853479862213, test loss:0.6175722479820251, test acc:0.8729
Epoch:124, training loss:0.013048660010099411, test loss:0.5081812739372253, test acc:0.8906
Epoch:125, training loss:0.03256283700466156, test loss:0.5165696740150452, test acc:0.8946
Epoch:126, training loss:0.013236886821687222, test loss:0.6402460932731628, test acc:0.873
Epoch:127, training loss:0.006677112076431513, test loss:0.5297825932502747, test acc:0.886
Epoch:128, training loss:0.00761262746527791, test loss:0.5210617780685425, test acc:0.8891
Epoch:129, training loss:0.00887613371014595, test loss:0.48001953959465027, test acc:0.8966
Epoch:130, training loss:0.05561057850718498, test loss:0.5808993577957153, test acc:0.8858
Epoch:131, training loss:0.02756565809249878, test loss:0.5158916115760803, test acc:0.8897
Epoch:132, training loss:0.02151910960674286, test loss:0.5430756211280823, test acc:0.8878
Epoch:133, training loss:0.007812919095158577, test loss:0.4855060577392578, test acc:0.8967
Epoch:134, training loss:0.003587583312764764, test loss:0.5070239901542664, test acc:0.8935
Epoch:135, training loss:0.008014035411179066, test loss:0.49311408400535583, test acc:0.8949
Epoch:136, training loss:0.017973555251955986, test loss:0.6063016057014465, test acc:0.8755
Epoch:137, training loss:0.005962422117590904, test loss:0.4966871738433838, test acc:0.8939
Epoch:138, training loss:0.028877222910523415, test loss:0.47719284892082214, test acc:0.9023
Epoch:139, training loss:0.010382700711488724, test loss:0.5607783198356628, test acc:0.8865
Epoch:140, training loss:0.009031129069626331, test loss:0.6224578619003296, test acc:0.8836
Epoch:141, training loss:0.03722737729549408, test loss:0.6123656034469604, test acc:0.8786
Epoch:142, training loss:0.03865037485957146, test loss:0.5647437572479248, test acc:0.8878
Epoch:143, training loss:0.006179327145218849, test loss:0.5475813746452332, test acc:0.8927
Epoch:144, training loss:0.0034590233117341995, test loss:0.554959237575531, test acc:0.8904
Epoch:145, training loss:0.05751529708504677, test loss:0.5316787362098694, test acc:0.8928
Epoch:146, training loss:0.006843617185950279, test loss:0.5859419703483582, test acc:0.8846
Epoch:147, training loss:0.027216756716370583, test loss:0.5053244233131409, test acc:0.897
Epoch:148, training loss:0.009001906029880047, test loss:0.5753633975982666, test acc:0.8897
Epoch:149, training loss:0.013932309113442898, test loss:0.5478967428207397, test acc:0.8922
Epoch:150, training loss:0.01248869113624096, test loss:0.5710384249687195, test acc:0.8892
Epoch:151, training loss:0.004130596294999123, test loss:0.5668277740478516, test acc:0.8907
Epoch:152, training loss:0.0065092844888567924, test loss:0.5950515866279602, test acc:0.8902
Epoch:153, training loss:0.004494663327932358, test loss:0.5859144926071167, test acc:0.8924
Epoch:154, training loss:0.0439147911965847, test loss:0.6986518502235413, test acc:0.8768
Epoch:155, training loss:0.01666460558772087, test loss:0.5342193245887756, test acc:0.8991
Epoch:156, training loss:0.0036601051688194275, test loss:0.5483773946762085, test acc:0.894
Epoch:157, training loss:0.030237622559070587, test loss:0.74810791015625, test acc:0.873
Epoch:158, training loss:0.013877974823117256, test loss:0.6753638386726379, test acc:0.875
Epoch:159, training loss:0.007352189160883427, test loss:0.6817449927330017, test acc:0.8785
Epoch:160, training loss:0.029380686581134796, test loss:0.643790066242218, test acc:0.8839
Epoch:161, training loss:0.0067221480421721935, test loss:0.6189759373664856, test acc:0.8936
Epoch:162, training loss:0.02674579992890358, test loss:0.5892317295074463, test acc:0.8902
Epoch:163, training loss:0.003657267428934574, test loss:0.6077130436897278, test acc:0.8893
Epoch:164, training loss:0.00431258138269186, test loss:0.7149994969367981, test acc:0.888
Epoch:165, training loss:0.004586661700159311, test loss:0.6161590218544006, test acc:0.8917
Epoch:166, training loss:0.015141711570322514, test loss:0.6393052339553833, test acc:0.8858
Epoch:167, training loss:0.003334813052788377, test loss:0.5941803455352783, test acc:0.8928
Epoch:168, training loss:0.025914467871189117, test loss:0.6188542246818542, test acc:0.8935
Epoch:169, training loss:0.004533527418971062, test loss:0.738811731338501, test acc:0.8736
Epoch:170, training loss:0.006512106861919165, test loss:0.648464024066925, test acc:0.8866
Epoch:171, training loss:0.018308058381080627, test loss:0.6936140060424805, test acc:0.8838
Epoch:172, training loss:0.0077830408699810505, test loss:0.649791419506073, test acc:0.8857
Epoch:173, training loss:0.005643127486109734, test loss:0.6345512866973877, test acc:0.8911
Epoch:174, training loss:0.02296079508960247, test loss:0.7406201362609863, test acc:0.8803
Epoch:175, training loss:0.010382939130067825, test loss:0.7422575354576111, test acc:0.8793
Epoch:176, training loss:0.00535792438313365, test loss:0.7244786620140076, test acc:0.8866
Epoch:177, training loss:0.0028550683055073023, test loss:0.7404530644416809, test acc:0.8848
Epoch:178, training loss:0.005729012191295624, test loss:0.6946229338645935, test acc:0.8849
Epoch:179, training loss:0.004869651049375534, test loss:0.6452301740646362, test acc:0.8894
Epoch:180, training loss:0.02004016749560833, test loss:0.6659987568855286, test acc:0.8904
Epoch:181, training loss:0.009176130406558514, test loss:0.6907956600189209, test acc:0.8906
Epoch:182, training loss:0.008133349008858204, test loss:0.6801103949546814, test acc:0.8865
Epoch:183, training loss:0.004847859963774681, test loss:0.7109044194221497, test acc:0.8875
Epoch:184, training loss:0.015540999360382557, test loss:0.7043172121047974, test acc:0.8862
Epoch:185, training loss:0.008855029009282589, test loss:0.6919443011283875, test acc:0.8885
Epoch:186, training loss:0.006382818333804607, test loss:0.6196295619010925, test acc:0.8976
Epoch:187, training loss:0.0018957611173391342, test loss:0.7661853432655334, test acc:0.8846
Epoch:188, training loss:0.004105277359485626, test loss:0.7154018878936768, test acc:0.8898
Epoch:189, training loss:0.018271690234541893, test loss:0.8000920414924622, test acc:0.8725
Epoch:190, training loss:0.004896718543022871, test loss:0.674373984336853, test acc:0.894
Epoch:191, training loss:0.0019787384662777185, test loss:0.7426213622093201, test acc:0.8871
Epoch:192, training loss:0.004393496084958315, test loss:0.7326058745384216, test acc:0.8877
Epoch:193, training loss:0.0008722902275621891, test loss:0.7120522856712341, test acc:0.8907
Epoch:194, training loss:0.03416253998875618, test loss:0.7757607102394104, test acc:0.8796
Epoch:195, training loss:0.00541352853178978, test loss:0.7219526767730713, test acc:0.8867
Epoch:196, training loss:0.005957572255283594, test loss:0.8024460673332214, test acc:0.8774
Epoch:197, training loss:0.008671439252793789, test loss:0.7843637466430664, test acc:0.8849
Epoch:198, training loss:0.004407328087836504, test loss:0.7998325228691101, test acc:0.884
Epoch:199, training loss:0.008180924691259861, test loss:0.7478299140930176, test acc:0.8858
Epoch:200, training loss:0.006505826488137245, test loss:0.709341287612915, test acc:0.8893
