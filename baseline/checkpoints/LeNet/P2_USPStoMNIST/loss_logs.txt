Begin training
Epoch:1, training loss:0.7043366432189941, test loss:0.7571129202842712, test acc:0.7423
Epoch:2, training loss:0.23996368050575256, test loss:0.5121347904205322, test acc:0.8333
Epoch:3, training loss:0.34395620226860046, test loss:0.42182809114456177, test acc:0.8672
Epoch:4, training loss:0.22980897128582, test loss:0.5506854057312012, test acc:0.8108
Epoch:5, training loss:0.14698857069015503, test loss:0.4083593487739563, test acc:0.8657
Epoch:6, training loss:0.1491381675004959, test loss:0.3240850865840912, test acc:0.8938
Epoch:7, training loss:0.0826280489563942, test loss:0.33215782046318054, test acc:0.8946
Epoch:8, training loss:0.17049039900302887, test loss:0.4102345108985901, test acc:0.8725
Epoch:9, training loss:0.07431844621896744, test loss:0.34965112805366516, test acc:0.8876
Epoch:10, training loss:0.0742831602692604, test loss:0.4798220694065094, test acc:0.8607
Epoch:11, training loss:0.04108583554625511, test loss:0.34229278564453125, test acc:0.9021
Epoch:12, training loss:0.07114734500646591, test loss:0.35111650824546814, test acc:0.9013
Epoch:13, training loss:0.11851557344198227, test loss:0.4019041657447815, test acc:0.8874
Epoch:14, training loss:0.014531445689499378, test loss:0.40469393134117126, test acc:0.8987
Epoch:15, training loss:0.08613426238298416, test loss:0.4081230163574219, test acc:0.8922
Epoch:16, training loss:0.05273214355111122, test loss:0.42870426177978516, test acc:0.8927
Epoch:17, training loss:0.025635847821831703, test loss:0.4850691854953766, test acc:0.8885
Epoch:18, training loss:0.02527974732220173, test loss:0.41830572485923767, test acc:0.8992
Epoch:19, training loss:0.026953207328915596, test loss:0.5346686244010925, test acc:0.8893
Epoch:20, training loss:0.04830403998494148, test loss:0.6329545378684998, test acc:0.865
Epoch:21, training loss:0.01664823107421398, test loss:0.48758140206336975, test acc:0.897
Epoch:22, training loss:0.022273046895861626, test loss:0.4586969316005707, test acc:0.9008
Epoch:23, training loss:0.03344385698437691, test loss:0.7354440689086914, test acc:0.8598
Epoch:24, training loss:0.0063934712670743465, test loss:0.5171398520469666, test acc:0.9053
Epoch:25, training loss:0.015193508006632328, test loss:0.5051167011260986, test acc:0.9015
Epoch:26, training loss:0.039139874279499054, test loss:0.45672306418418884, test acc:0.9129
Epoch:27, training loss:0.012322906404733658, test loss:0.5646888613700867, test acc:0.8948
Epoch:28, training loss:0.011977707035839558, test loss:0.7715151309967041, test acc:0.8668
Epoch:29, training loss:0.0037850888911634684, test loss:0.5497957468032837, test acc:0.9058
Epoch:30, training loss:0.01423994917422533, test loss:0.5812944769859314, test acc:0.8964
Epoch:31, training loss:0.001889095059596002, test loss:0.5564996600151062, test acc:0.8994
Epoch:32, training loss:0.018934963271021843, test loss:0.7787627577781677, test acc:0.8766
Epoch:33, training loss:0.03883741796016693, test loss:0.5197809338569641, test acc:0.9079
Epoch:34, training loss:0.002987842308357358, test loss:0.5128470063209534, test acc:0.9131
Epoch:35, training loss:0.03854239359498024, test loss:0.6949016451835632, test acc:0.8855
Epoch:36, training loss:0.0022852595429867506, test loss:0.6881692409515381, test acc:0.8977
Epoch:37, training loss:0.0012300265952944756, test loss:0.6737474203109741, test acc:0.8977
Epoch:38, training loss:0.0006332087214104831, test loss:0.9550876021385193, test acc:0.8714
Epoch:39, training loss:0.0003579690237529576, test loss:0.7608704566955566, test acc:0.8948
Epoch:40, training loss:0.002109888009727001, test loss:0.7884449362754822, test acc:0.8771
Epoch:41, training loss:0.0005006305291317403, test loss:0.7890005707740784, test acc:0.8939
Epoch:42, training loss:0.0012596012093126774, test loss:0.7183933258056641, test acc:0.9003
Epoch:43, training loss:0.002070232992991805, test loss:0.8468016982078552, test acc:0.8929
Epoch:44, training loss:0.009552501142024994, test loss:0.9031491279602051, test acc:0.8819
Epoch:45, training loss:0.0012558882590383291, test loss:1.0991908311843872, test acc:0.8712
Epoch:46, training loss:0.004602854605764151, test loss:0.7232280373573303, test acc:0.8996
Epoch:47, training loss:0.07689269632101059, test loss:0.899488627910614, test acc:0.873
Epoch:48, training loss:0.00043972721323370934, test loss:0.9182562828063965, test acc:0.89
Epoch:49, training loss:4.1523599065840244e-05, test loss:0.9538006782531738, test acc:0.8833
Epoch:50, training loss:0.0001383506169077009, test loss:0.912664532661438, test acc:0.8861
Epoch:51, training loss:0.0013684441801160574, test loss:0.9714986681938171, test acc:0.8875
Epoch:52, training loss:0.002661776728928089, test loss:1.02252995967865, test acc:0.8831
Epoch:53, training loss:0.0005883395206183195, test loss:1.0667667388916016, test acc:0.8725
Epoch:54, training loss:0.007546893786638975, test loss:0.99092036485672, test acc:0.8884
Epoch:55, training loss:0.007596215698868036, test loss:1.200982928276062, test acc:0.8748
Epoch:56, training loss:0.00021015337551943958, test loss:0.9469739198684692, test acc:0.8913
Epoch:57, training loss:0.00012543531192932278, test loss:0.9944736361503601, test acc:0.8916
Epoch:58, training loss:0.00045150372898206115, test loss:1.1592628955841064, test acc:0.874
Epoch:59, training loss:4.946700937580317e-05, test loss:1.0052530765533447, test acc:0.8918
Epoch:60, training loss:0.0005980933783575892, test loss:1.0197614431381226, test acc:0.8881
Epoch:61, training loss:0.00011720308248186484, test loss:0.9874384999275208, test acc:0.8909
Epoch:62, training loss:0.0009874444222077727, test loss:0.970916211605072, test acc:0.8935
Epoch:63, training loss:0.023027410730719566, test loss:1.3097789287567139, test acc:0.8735
Epoch:64, training loss:0.0006120289908722043, test loss:1.2613331079483032, test acc:0.8794
Epoch:65, training loss:0.00011369658750481904, test loss:1.0874629020690918, test acc:0.8795
Epoch:66, training loss:0.000828475458547473, test loss:1.3351153135299683, test acc:0.8779
Epoch:67, training loss:0.0013990706065669656, test loss:1.107686161994934, test acc:0.8919
Epoch:68, training loss:0.030585942789912224, test loss:1.0890499353408813, test acc:0.8925
Epoch:69, training loss:0.0003847969346679747, test loss:1.1166661977767944, test acc:0.8867
Epoch:70, training loss:0.02296060137450695, test loss:1.1518712043762207, test acc:0.8908
Epoch:71, training loss:0.0024404118303209543, test loss:1.1275982856750488, test acc:0.8857
Epoch:72, training loss:0.0004687062173616141, test loss:1.2051907777786255, test acc:0.8869
Epoch:73, training loss:0.0007910873973742127, test loss:1.2159870862960815, test acc:0.8926
Epoch:74, training loss:5.309950211085379e-05, test loss:1.2311497926712036, test acc:0.8768
Epoch:75, training loss:4.568526492221281e-05, test loss:1.1324677467346191, test acc:0.8844
Epoch:76, training loss:1.1416955203458201e-06, test loss:1.0177749395370483, test acc:0.8948
Epoch:77, training loss:3.0385768695850857e-05, test loss:1.0940676927566528, test acc:0.8905
Epoch:78, training loss:6.85017266732757e-06, test loss:1.1615386009216309, test acc:0.8916
Epoch:79, training loss:0.00013634635251946747, test loss:1.0457271337509155, test acc:0.8914
Epoch:80, training loss:0.04986274614930153, test loss:1.3969171047210693, test acc:0.8766
Epoch:81, training loss:0.0007959672366268933, test loss:1.1654704809188843, test acc:0.8889
Epoch:82, training loss:7.079868737491779e-06, test loss:1.222057819366455, test acc:0.8894
Epoch:83, training loss:0.0010429291287437081, test loss:1.3025293350219727, test acc:0.8838
Epoch:84, training loss:5.099831469124183e-06, test loss:1.3290404081344604, test acc:0.8789
Epoch:85, training loss:0.0008652223623357713, test loss:1.4896719455718994, test acc:0.8677
Epoch:86, training loss:2.323714943486266e-05, test loss:1.220741868019104, test acc:0.892
Epoch:87, training loss:5.1475151849444956e-05, test loss:1.250250220298767, test acc:0.895
Epoch:88, training loss:5.446798354569182e-07, test loss:1.2196043729782104, test acc:0.8915
Epoch:89, training loss:0.0006822977447882295, test loss:1.299458384513855, test acc:0.8909
Epoch:90, training loss:0.0002508255420252681, test loss:1.3931618928909302, test acc:0.8866
Epoch:91, training loss:0.0011120024137198925, test loss:1.3702329397201538, test acc:0.8886
Epoch:92, training loss:1.3751197002420668e-05, test loss:1.3404098749160767, test acc:0.8916
Epoch:93, training loss:4.640100451069884e-05, test loss:1.2729172706604004, test acc:0.8886
Epoch:94, training loss:0.00010772242967505008, test loss:1.4574960470199585, test acc:0.8876
Epoch:95, training loss:9.56194162426982e-06, test loss:1.4085618257522583, test acc:0.8847
Epoch:96, training loss:1.2453978115445352e-06, test loss:1.376848816871643, test acc:0.8888
Epoch:97, training loss:0.0001868746185209602, test loss:1.5724180936813354, test acc:0.8779
Epoch:98, training loss:0.0003686509735416621, test loss:1.2760823965072632, test acc:0.8893
Epoch:99, training loss:2.41810721490765e-06, test loss:1.2756682634353638, test acc:0.8879
Epoch:100, training loss:1.9170404357282678e-06, test loss:1.3266414403915405, test acc:0.8882
Epoch:101, training loss:2.321188958376297e-06, test loss:1.389045238494873, test acc:0.8893
