Begin training
Epoch:1, training loss:2.2975096702575684, test loss:2.2955050468444824, test acc:0.1745
Epoch:2, training loss:2.2853143215179443, test loss:2.2816038131713867, test acc:0.285
Epoch:3, training loss:2.242434501647949, test loss:2.255795478820801, test acc:0.4605
Epoch:4, training loss:2.1930055618286133, test loss:2.2144250869750977, test acc:0.4645
Epoch:5, training loss:2.102893114089966, test loss:2.1568894386291504, test acc:0.4265
Epoch:6, training loss:2.0787172317504883, test loss:2.0817782878875732, test acc:0.554
Epoch:7, training loss:2.116523027420044, test loss:1.9927361011505127, test acc:0.6
Epoch:8, training loss:1.7384815216064453, test loss:1.89476478099823, test acc:0.525
Epoch:9, training loss:1.7212982177734375, test loss:1.7841150760650635, test acc:0.572
Epoch:10, training loss:1.394579291343689, test loss:1.665010929107666, test acc:0.5945
Epoch:11, training loss:1.392293095588684, test loss:1.561326026916504, test acc:0.6415
Epoch:12, training loss:1.4613635540008545, test loss:1.4633562564849854, test acc:0.604
Epoch:13, training loss:0.7882994413375854, test loss:1.3541617393493652, test acc:0.6255
Epoch:14, training loss:1.2060225009918213, test loss:1.3087126016616821, test acc:0.605
Epoch:15, training loss:0.7950135469436646, test loss:1.2171473503112793, test acc:0.633
Epoch:16, training loss:0.8011022210121155, test loss:1.1633906364440918, test acc:0.643
Epoch:17, training loss:0.7575830817222595, test loss:1.1191350221633911, test acc:0.639
Epoch:18, training loss:1.0820631980895996, test loss:1.074129581451416, test acc:0.654
Epoch:19, training loss:0.9650691151618958, test loss:1.0968294143676758, test acc:0.6425
Epoch:20, training loss:1.2533164024353027, test loss:1.0122483968734741, test acc:0.6765
Epoch:21, training loss:0.7415033578872681, test loss:1.033614993095398, test acc:0.66
Epoch:22, training loss:0.2732880413532257, test loss:0.9553248882293701, test acc:0.684
Epoch:23, training loss:0.7406162619590759, test loss:0.9538260698318481, test acc:0.6815
Epoch:24, training loss:0.4039246737957001, test loss:0.9763825535774231, test acc:0.673
Epoch:25, training loss:0.6494958400726318, test loss:0.9840668439865112, test acc:0.666
Epoch:26, training loss:1.0536010265350342, test loss:1.0488348007202148, test acc:0.651
Epoch:27, training loss:0.7929888367652893, test loss:0.9734222292900085, test acc:0.676
Epoch:28, training loss:0.6392272710800171, test loss:0.8889588117599487, test acc:0.714
Epoch:29, training loss:0.9016093015670776, test loss:1.0048809051513672, test acc:0.6685
Epoch:30, training loss:1.1537775993347168, test loss:0.9142181873321533, test acc:0.702
Epoch:31, training loss:0.16626816987991333, test loss:0.9749630093574524, test acc:0.673
Epoch:32, training loss:0.5276126265525818, test loss:0.8951055407524109, test acc:0.7095
Epoch:33, training loss:0.536434531211853, test loss:1.0475913286209106, test acc:0.6605
Epoch:34, training loss:0.22701871395111084, test loss:0.9373118877410889, test acc:0.6825
Epoch:35, training loss:0.3247710168361664, test loss:0.9789335131645203, test acc:0.6785
Epoch:36, training loss:0.22379335761070251, test loss:0.8768301606178284, test acc:0.712
Epoch:37, training loss:0.5560401082038879, test loss:0.8408953547477722, test acc:0.7285
Epoch:38, training loss:1.1101365089416504, test loss:1.1256245374679565, test acc:0.6345
Epoch:39, training loss:0.30790582299232483, test loss:0.8574303984642029, test acc:0.7115
Epoch:40, training loss:0.5565389394760132, test loss:1.0383098125457764, test acc:0.6635
Epoch:41, training loss:0.6463852524757385, test loss:0.9319053888320923, test acc:0.691
Epoch:42, training loss:0.23725491762161255, test loss:0.9512081146240234, test acc:0.682
Epoch:43, training loss:1.0646718740463257, test loss:1.104235291481018, test acc:0.644
Epoch:44, training loss:0.442163348197937, test loss:0.932757556438446, test acc:0.6845
Epoch:45, training loss:0.4475621283054352, test loss:0.970780611038208, test acc:0.6835
Epoch:46, training loss:0.48078230023384094, test loss:0.968559980392456, test acc:0.679
Epoch:47, training loss:0.4132898449897766, test loss:1.2052398920059204, test acc:0.616
Epoch:48, training loss:0.16016346216201782, test loss:1.069421410560608, test acc:0.6555
Epoch:49, training loss:0.4313325881958008, test loss:0.8867013454437256, test acc:0.7105
Epoch:50, training loss:0.7151901721954346, test loss:0.9413332939147949, test acc:0.6935
Epoch:51, training loss:0.04014533758163452, test loss:0.9850437641143799, test acc:0.6825
Epoch:52, training loss:0.15674400329589844, test loss:1.059755802154541, test acc:0.658
Epoch:53, training loss:0.36858993768692017, test loss:0.9504318833351135, test acc:0.6905
Epoch:54, training loss:0.10400855541229248, test loss:1.035319209098816, test acc:0.6785
Epoch:55, training loss:0.5360383987426758, test loss:1.121401071548462, test acc:0.649
Epoch:56, training loss:0.08392065763473511, test loss:1.0361961126327515, test acc:0.679
Epoch:57, training loss:0.3241010904312134, test loss:0.9877246022224426, test acc:0.693
Epoch:58, training loss:0.45039862394332886, test loss:1.0425009727478027, test acc:0.674
Epoch:59, training loss:0.30197954177856445, test loss:1.1044329404830933, test acc:0.656
Epoch:60, training loss:0.2504294812679291, test loss:1.0283604860305786, test acc:0.683
Epoch:61, training loss:0.30567342042922974, test loss:1.1649624109268188, test acc:0.6475
Epoch:62, training loss:0.6496508121490479, test loss:1.3130582571029663, test acc:0.615
Epoch:63, training loss:0.41705650091171265, test loss:1.1167562007904053, test acc:0.659
Epoch:64, training loss:0.15021485090255737, test loss:1.1223599910736084, test acc:0.6515
Epoch:65, training loss:0.40447065234184265, test loss:1.2001681327819824, test acc:0.642
Epoch:66, training loss:0.1854758858680725, test loss:1.025867223739624, test acc:0.682
Epoch:67, training loss:0.06868213415145874, test loss:1.0347498655319214, test acc:0.681
Epoch:68, training loss:0.6016831398010254, test loss:1.1713738441467285, test acc:0.647
Epoch:69, training loss:0.25837114453315735, test loss:1.192685842514038, test acc:0.649
Epoch:70, training loss:0.15008682012557983, test loss:1.1649388074874878, test acc:0.649
Epoch:71, training loss:0.47255927324295044, test loss:1.1555811166763306, test acc:0.6565
Epoch:72, training loss:0.5808656215667725, test loss:1.2427160739898682, test acc:0.632
Epoch:73, training loss:0.1193189024925232, test loss:1.0006521940231323, test acc:0.7005
Epoch:74, training loss:0.4959280490875244, test loss:1.0366325378417969, test acc:0.683
Epoch:75, training loss:0.5950294733047485, test loss:1.2089853286743164, test acc:0.6495
Epoch:76, training loss:0.11854654550552368, test loss:1.1176735162734985, test acc:0.676
Epoch:77, training loss:0.594568133354187, test loss:1.2472343444824219, test acc:0.646
Epoch:78, training loss:0.7195531129837036, test loss:1.1149057149887085, test acc:0.671
Epoch:79, training loss:0.2941763401031494, test loss:1.3300011157989502, test acc:0.626
Epoch:80, training loss:0.2747410833835602, test loss:1.0978388786315918, test acc:0.6755
Epoch:81, training loss:0.6247467398643494, test loss:1.1717028617858887, test acc:0.6665
Epoch:82, training loss:0.03075563907623291, test loss:1.08928382396698, test acc:0.6835
Epoch:83, training loss:0.16121292114257812, test loss:1.130768060684204, test acc:0.674
Epoch:84, training loss:0.12184259295463562, test loss:1.0849581956863403, test acc:0.679
Epoch:85, training loss:0.19982904195785522, test loss:1.169955849647522, test acc:0.677
Epoch:86, training loss:0.05407238006591797, test loss:1.2470707893371582, test acc:0.6555
Epoch:87, training loss:0.27551043033599854, test loss:1.4796503782272339, test acc:0.6125
Epoch:88, training loss:0.0511474609375, test loss:1.1405208110809326, test acc:0.679
Epoch:89, training loss:0.10098928213119507, test loss:1.1932666301727295, test acc:0.6715
Epoch:90, training loss:0.6092245578765869, test loss:1.1036053895950317, test acc:0.692
Epoch:91, training loss:0.573634684085846, test loss:1.4336518049240112, test acc:0.628
Epoch:92, training loss:0.06068074703216553, test loss:1.1870546340942383, test acc:0.6755
Epoch:93, training loss:0.015638291835784912, test loss:1.1774680614471436, test acc:0.676
Epoch:94, training loss:0.09024512767791748, test loss:1.086937665939331, test acc:0.7005
Epoch:95, training loss:0.042345523834228516, test loss:1.1412416696548462, test acc:0.6925
Epoch:96, training loss:0.09775352478027344, test loss:1.0937442779541016, test acc:0.7055
Epoch:97, training loss:0.3028375506401062, test loss:1.1356651782989502, test acc:0.6995
Epoch:98, training loss:0.3605765700340271, test loss:1.1514009237289429, test acc:0.697
Epoch:99, training loss:0.2800759971141815, test loss:1.3359100818634033, test acc:0.652
Epoch:100, training loss:0.05419892072677612, test loss:1.149547815322876, test acc:0.6935
Epoch:101, training loss:0.3460220694541931, test loss:1.1635143756866455, test acc:0.689
Epoch:102, training loss:0.22645172476768494, test loss:1.1892728805541992, test acc:0.688
Epoch:103, training loss:0.30548757314682007, test loss:1.2972936630249023, test acc:0.6725
Epoch:104, training loss:0.14733117818832397, test loss:1.181050181388855, test acc:0.6925
Epoch:105, training loss:0.08050012588500977, test loss:1.1792106628417969, test acc:0.6935
Epoch:106, training loss:0.084723562002182, test loss:1.1293691396713257, test acc:0.698
Epoch:107, training loss:0.10530626773834229, test loss:1.1759575605392456, test acc:0.6995
Epoch:108, training loss:0.13335007429122925, test loss:1.3956611156463623, test acc:0.653
Epoch:109, training loss:0.2874995470046997, test loss:1.1821680068969727, test acc:0.6995
Epoch:110, training loss:0.28881555795669556, test loss:1.4600192308425903, test acc:0.6335
Epoch:111, training loss:0.23209106922149658, test loss:1.2250558137893677, test acc:0.6805
Epoch:112, training loss:0.12439405918121338, test loss:1.3552496433258057, test acc:0.6705
Epoch:113, training loss:0.13191157579421997, test loss:1.3009278774261475, test acc:0.668
Epoch:114, training loss:0.1963198184967041, test loss:1.3830180168151855, test acc:0.6655
Epoch:115, training loss:0.08510959148406982, test loss:1.2439974546432495, test acc:0.677
Epoch:116, training loss:0.17831504344940186, test loss:1.252054214477539, test acc:0.686
Epoch:117, training loss:0.2817506194114685, test loss:1.2753514051437378, test acc:0.6895
Epoch:118, training loss:0.03925532102584839, test loss:1.3492484092712402, test acc:0.6675
Epoch:119, training loss:0.18603560328483582, test loss:1.291825294494629, test acc:0.6885
Epoch:120, training loss:0.08356165885925293, test loss:1.3635936975479126, test acc:0.6685
Epoch:121, training loss:0.1474786400794983, test loss:1.6495916843414307, test acc:0.615
Epoch:122, training loss:0.08717888593673706, test loss:1.2695282697677612, test acc:0.695
Epoch:123, training loss:0.12506455183029175, test loss:1.3043402433395386, test acc:0.6925
Epoch:124, training loss:0.13245177268981934, test loss:1.6317789554595947, test acc:0.6325
Epoch:125, training loss:0.23489731550216675, test loss:1.4030790328979492, test acc:0.6505
Epoch:126, training loss:0.021936893463134766, test loss:1.443079948425293, test acc:0.66
Epoch:127, training loss:0.08707928657531738, test loss:1.267421007156372, test acc:0.696
Epoch:128, training loss:0.13588696718215942, test loss:1.4101617336273193, test acc:0.679
Epoch:129, training loss:0.5063436031341553, test loss:1.3988136053085327, test acc:0.6685
Epoch:130, training loss:0.2184206247329712, test loss:1.4060086011886597, test acc:0.678
Epoch:131, training loss:0.3017202615737915, test loss:1.1459674835205078, test acc:0.7205
Epoch:132, training loss:0.2533659338951111, test loss:1.572469711303711, test acc:0.6425
Epoch:133, training loss:0.09715008735656738, test loss:1.5470175743103027, test acc:0.6415
Epoch:134, training loss:0.18599724769592285, test loss:1.3550405502319336, test acc:0.687
Epoch:135, training loss:0.2707148790359497, test loss:1.4606125354766846, test acc:0.669
Epoch:136, training loss:0.1949855089187622, test loss:1.3572086095809937, test acc:0.689
Epoch:137, training loss:0.06358623504638672, test loss:1.4443342685699463, test acc:0.6635
Epoch:138, training loss:0.221349835395813, test loss:1.4688866138458252, test acc:0.653
Epoch:139, training loss:0.5188568234443665, test loss:1.3785605430603027, test acc:0.6905
Epoch:140, training loss:0.10589957237243652, test loss:1.4884765148162842, test acc:0.6585
Epoch:141, training loss:0.03290468454360962, test loss:1.5318018198013306, test acc:0.663
Epoch:142, training loss:0.01989215612411499, test loss:1.3872133493423462, test acc:0.683
Epoch:143, training loss:0.0884627103805542, test loss:1.3319664001464844, test acc:0.6825
Epoch:144, training loss:0.07869714498519897, test loss:1.5020180940628052, test acc:0.6725
Epoch:145, training loss:0.09722638130187988, test loss:1.6339569091796875, test acc:0.634
Epoch:146, training loss:0.0879639983177185, test loss:1.5707894563674927, test acc:0.647
Epoch:147, training loss:0.3855956494808197, test loss:1.6256165504455566, test acc:0.644
Epoch:148, training loss:0.0410541296005249, test loss:1.4373347759246826, test acc:0.675
Epoch:149, training loss:0.21575239300727844, test loss:1.3831044435501099, test acc:0.701
Epoch:150, training loss:0.4279990792274475, test loss:1.4596725702285767, test acc:0.661
Epoch:151, training loss:0.9238302707672119, test loss:1.4559741020202637, test acc:0.68
Epoch:152, training loss:0.09789592027664185, test loss:1.4129077196121216, test acc:0.677
Epoch:153, training loss:0.13010066747665405, test loss:1.3942394256591797, test acc:0.681
Epoch:154, training loss:0.02006453275680542, test loss:1.3536077737808228, test acc:0.6885
Epoch:155, training loss:0.03840911388397217, test loss:1.4302349090576172, test acc:0.6785
Epoch:156, training loss:0.14835554361343384, test loss:1.3394229412078857, test acc:0.6835
Epoch:157, training loss:0.2756275534629822, test loss:1.9034924507141113, test acc:0.6175
Epoch:158, training loss:0.08357387781143188, test loss:1.4027564525604248, test acc:0.6925
Epoch:159, training loss:0.21053004264831543, test loss:1.2606796026229858, test acc:0.717
Epoch:160, training loss:0.5705003142356873, test loss:1.5313228368759155, test acc:0.678
Epoch:161, training loss:0.5160794258117676, test loss:1.3909845352172852, test acc:0.692
Epoch:162, training loss:0.11766576766967773, test loss:1.3496754169464111, test acc:0.703
Epoch:163, training loss:0.0675925612449646, test loss:1.4654585123062134, test acc:0.6765
Epoch:164, training loss:0.1655016541481018, test loss:1.578715443611145, test acc:0.668
Epoch:165, training loss:0.05073672533035278, test loss:1.4377772808074951, test acc:0.6865
Epoch:166, training loss:0.04706776142120361, test loss:1.4717167615890503, test acc:0.6805
Epoch:167, training loss:0.1372697949409485, test loss:1.650292158126831, test acc:0.656
Epoch:168, training loss:0.05864262580871582, test loss:1.570842981338501, test acc:0.677
Epoch:169, training loss:0.05081743001937866, test loss:1.488964557647705, test acc:0.6915
Epoch:170, training loss:0.025542497634887695, test loss:1.411273717880249, test acc:0.7015
Epoch:171, training loss:0.15689897537231445, test loss:1.603948950767517, test acc:0.6675
Epoch:172, training loss:0.04456603527069092, test loss:1.8287644386291504, test acc:0.6395
Epoch:173, training loss:0.11365514993667603, test loss:1.6478983163833618, test acc:0.6705
Epoch:174, training loss:0.12375777959823608, test loss:1.4713743925094604, test acc:0.6875
Epoch:175, training loss:0.2559143602848053, test loss:1.8974478244781494, test acc:0.6255
Epoch:176, training loss:0.0600740909576416, test loss:1.5442166328430176, test acc:0.685
Epoch:177, training loss:0.17232179641723633, test loss:1.5379159450531006, test acc:0.6845
Epoch:178, training loss:0.03654074668884277, test loss:1.505044937133789, test acc:0.6855
Epoch:179, training loss:0.04471898078918457, test loss:1.4954689741134644, test acc:0.685
Epoch:180, training loss:0.06722134351730347, test loss:1.7369498014450073, test acc:0.657
Epoch:181, training loss:0.11608767509460449, test loss:1.5185424089431763, test acc:0.694
Epoch:182, training loss:0.042181968688964844, test loss:1.6614508628845215, test acc:0.668
Epoch:183, training loss:0.019139587879180908, test loss:1.4816510677337646, test acc:0.7025
Epoch:184, training loss:0.02252817153930664, test loss:1.431813359260559, test acc:0.712
Epoch:185, training loss:0.007616698741912842, test loss:1.6125531196594238, test acc:0.6715
Epoch:186, training loss:0.0855187177658081, test loss:1.8026620149612427, test acc:0.6545
Epoch:187, training loss:0.033922791481018066, test loss:1.5790889263153076, test acc:0.688
Epoch:188, training loss:0.04759734869003296, test loss:1.7307791709899902, test acc:0.663
Epoch:189, training loss:0.0063364505767822266, test loss:1.9151999950408936, test acc:0.6385
Epoch:190, training loss:0.10684210062026978, test loss:2.0068063735961914, test acc:0.6255
Epoch:191, training loss:0.2248755693435669, test loss:1.9842747449874878, test acc:0.626
Epoch:192, training loss:0.40797168016433716, test loss:1.405216097831726, test acc:0.7175
Epoch:193, training loss:0.016746103763580322, test loss:1.61323881149292, test acc:0.679
Epoch:194, training loss:0.11749815940856934, test loss:1.592395544052124, test acc:0.6805
Epoch:195, training loss:0.11855220794677734, test loss:1.7808259725570679, test acc:0.665
Epoch:196, training loss:0.004532337188720703, test loss:1.593172311782837, test acc:0.69
Epoch:197, training loss:0.01699727773666382, test loss:1.893646478652954, test acc:0.6535
Epoch:198, training loss:0.24317193031311035, test loss:1.585783839225769, test acc:0.696
Epoch:199, training loss:0.16445016860961914, test loss:1.691495418548584, test acc:0.6855
Epoch:200, training loss:0.001321256160736084, test loss:1.585749864578247, test acc:0.6925
