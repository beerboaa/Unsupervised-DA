Begin training
Epoch:1, training loss:2.076714515686035, test loss:2.080942153930664, test acc:0.3055
Epoch:2, training loss:1.324497103691101, test loss:1.9380159378051758, test acc:0.409
Epoch:3, training loss:0.4889560341835022, test loss:1.773477554321289, test acc:0.454
Epoch:4, training loss:0.6230705976486206, test loss:2.0128302574157715, test acc:0.384
Epoch:5, training loss:0.689394474029541, test loss:1.036630630493164, test acc:0.6545
Epoch:6, training loss:0.42821115255355835, test loss:1.0550382137298584, test acc:0.6545
Epoch:7, training loss:0.5794707536697388, test loss:1.0338451862335205, test acc:0.69
Epoch:8, training loss:0.1616983413696289, test loss:0.9454704523086548, test acc:0.7065
Epoch:9, training loss:0.8244964480400085, test loss:1.7660307884216309, test acc:0.55
Epoch:10, training loss:0.12425410747528076, test loss:1.0148651599884033, test acc:0.6985
Epoch:11, training loss:0.2661718428134918, test loss:0.8359125852584839, test acc:0.7365
Epoch:12, training loss:0.2828044295310974, test loss:1.1661676168441772, test acc:0.6715
Epoch:13, training loss:0.04401916265487671, test loss:1.2678369283676147, test acc:0.667
Epoch:14, training loss:0.12613677978515625, test loss:1.6311182975769043, test acc:0.5985
Epoch:15, training loss:0.15065866708755493, test loss:1.8828816413879395, test acc:0.5715
Epoch:16, training loss:0.051087915897369385, test loss:0.9342180490493774, test acc:0.741
Epoch:17, training loss:0.005066514015197754, test loss:1.0947716236114502, test acc:0.7185
Epoch:18, training loss:0.05448490381240845, test loss:1.9082835912704468, test acc:0.5735
Epoch:19, training loss:0.02919536828994751, test loss:1.040274977684021, test acc:0.7315
Epoch:20, training loss:0.4050401449203491, test loss:2.5933494567871094, test acc:0.553
Epoch:21, training loss:0.08562105894088745, test loss:1.0051286220550537, test acc:0.7295
Epoch:22, training loss:0.06655406951904297, test loss:1.1610221862792969, test acc:0.7265
Epoch:23, training loss:0.031396687030792236, test loss:1.5122038125991821, test acc:0.677
Epoch:24, training loss:0.06615814566612244, test loss:1.5752089023590088, test acc:0.6825
Epoch:25, training loss:0.45116567611694336, test loss:1.6923513412475586, test acc:0.6455
Epoch:26, training loss:0.21272575855255127, test loss:1.113574743270874, test acc:0.723
Epoch:27, training loss:0.020544111728668213, test loss:1.5030503273010254, test acc:0.6775
Epoch:28, training loss:0.026972651481628418, test loss:1.443647861480713, test acc:0.6765
Epoch:29, training loss:0.7210200428962708, test loss:1.663569450378418, test acc:0.6595
Epoch:30, training loss:0.005593299865722656, test loss:1.2266032695770264, test acc:0.7435
Epoch:31, training loss:0.014740526676177979, test loss:1.0530776977539062, test acc:0.766
Epoch:32, training loss:0.13615185022354126, test loss:1.4694303274154663, test acc:0.7175
Epoch:33, training loss:0.24701642990112305, test loss:1.519594430923462, test acc:0.705
Epoch:34, training loss:0.12851089239120483, test loss:1.3329085111618042, test acc:0.7265
Epoch:35, training loss:0.3207802176475525, test loss:1.5045135021209717, test acc:0.6975
Epoch:36, training loss:0.042557597160339355, test loss:1.2118369340896606, test acc:0.736
Epoch:37, training loss:0.013851463794708252, test loss:1.7377228736877441, test acc:0.6925
Epoch:38, training loss:0.03878754377365112, test loss:1.3923134803771973, test acc:0.745
Epoch:39, training loss:0.037403225898742676, test loss:1.9992178678512573, test acc:0.6755
Epoch:40, training loss:0.0017173290252685547, test loss:1.8526238203048706, test acc:0.692
Epoch:41, training loss:0.028411448001861572, test loss:1.634560465812683, test acc:0.706
Epoch:42, training loss:0.003110527992248535, test loss:1.9240360260009766, test acc:0.689
Epoch:43, training loss:6.759166717529297e-05, test loss:1.6402301788330078, test acc:0.7345
Epoch:44, training loss:0.0009933710098266602, test loss:1.5401791334152222, test acc:0.755
Epoch:45, training loss:0.03560495376586914, test loss:1.6153844594955444, test acc:0.7355
Epoch:46, training loss:0.007277846336364746, test loss:1.956814169883728, test acc:0.7005
Epoch:47, training loss:0.03537452220916748, test loss:2.042417526245117, test acc:0.6835
Epoch:48, training loss:0.09460267424583435, test loss:2.6180880069732666, test acc:0.6505
Epoch:49, training loss:0.0044741034507751465, test loss:1.6361334323883057, test acc:0.742
Epoch:50, training loss:0.0006986856460571289, test loss:2.311697006225586, test acc:0.6805
Epoch:51, training loss:0.0016744136810302734, test loss:2.1944849491119385, test acc:0.691
Epoch:52, training loss:2.47955322265625e-05, test loss:2.0254812240600586, test acc:0.7175
Epoch:53, training loss:0.020589470863342285, test loss:2.1436195373535156, test acc:0.7235
Epoch:54, training loss:0.012792587280273438, test loss:2.2509994506835938, test acc:0.684
Epoch:55, training loss:0.0057305097579956055, test loss:2.1808977127075195, test acc:0.7065
Epoch:56, training loss:0.0022062063217163086, test loss:2.3841445446014404, test acc:0.704
Epoch:57, training loss:0.00013446807861328125, test loss:2.2633535861968994, test acc:0.729
Epoch:58, training loss:0.11653962731361389, test loss:2.6393237113952637, test acc:0.6675
Epoch:59, training loss:0.002448439598083496, test loss:2.0742664337158203, test acc:0.724
Epoch:60, training loss:0.011436223983764648, test loss:2.050722599029541, test acc:0.742
Epoch:61, training loss:0.01407778263092041, test loss:2.0358965396881104, test acc:0.7475
Epoch:62, training loss:0.0023336410522460938, test loss:2.2495980262756348, test acc:0.7195
Epoch:63, training loss:0.000670313835144043, test loss:2.4377899169921875, test acc:0.715
Epoch:64, training loss:0.23689013719558716, test loss:1.878588080406189, test acc:0.755
Epoch:65, training loss:0.0011310577392578125, test loss:1.905083179473877, test acc:0.7565
Epoch:66, training loss:0.10010236501693726, test loss:3.186457633972168, test acc:0.6695
Epoch:67, training loss:0.00030422210693359375, test loss:2.059018850326538, test acc:0.748
Epoch:68, training loss:0.0023424625396728516, test loss:2.217437267303467, test acc:0.7285
Epoch:69, training loss:0.000550389289855957, test loss:2.3487350940704346, test acc:0.731
Epoch:70, training loss:0.0007174015045166016, test loss:2.4720282554626465, test acc:0.729
Epoch:71, training loss:0.00015795230865478516, test loss:2.1482508182525635, test acc:0.7395
Epoch:72, training loss:7.95125961303711e-05, test loss:2.3801426887512207, test acc:0.732
Epoch:73, training loss:6.67572021484375e-05, test loss:2.4048495292663574, test acc:0.7465
Epoch:74, training loss:0.0002865791320800781, test loss:2.8440496921539307, test acc:0.703
Epoch:75, training loss:0.02215898036956787, test loss:2.6781015396118164, test acc:0.6945
Epoch:76, training loss:0.002545595169067383, test loss:2.6883225440979004, test acc:0.7235
Epoch:77, training loss:8.678436279296875e-05, test loss:2.665760040283203, test acc:0.7325
Epoch:78, training loss:1.049041748046875e-05, test loss:2.6772749423980713, test acc:0.715
Epoch:79, training loss:0.0007718801498413086, test loss:2.823219060897827, test acc:0.7115
Epoch:80, training loss:0.02641606330871582, test loss:2.69939923286438, test acc:0.712
Epoch:81, training loss:6.99758529663086e-05, test loss:3.0651793479919434, test acc:0.701
Epoch:82, training loss:2.1696090698242188e-05, test loss:2.990859031677246, test acc:0.706
Epoch:83, training loss:0.009996891021728516, test loss:3.176854133605957, test acc:0.6965
Epoch:84, training loss:9.5367431640625e-06, test loss:3.0412027835845947, test acc:0.718
Epoch:85, training loss:0.0018187761306762695, test loss:3.1757991313934326, test acc:0.7085
Epoch:86, training loss:5.6624412536621094e-05, test loss:3.08537220954895, test acc:0.715
Epoch:87, training loss:0.0002263784408569336, test loss:2.795106887817383, test acc:0.722
Epoch:88, training loss:0.007947325706481934, test loss:3.1110856533050537, test acc:0.7235
Epoch:89, training loss:0.1214601993560791, test loss:2.800137519836426, test acc:0.727
Epoch:90, training loss:0.00011414289474487305, test loss:3.013941526412964, test acc:0.7145
Epoch:91, training loss:0.0038657188415527344, test loss:3.1298301219940186, test acc:0.709
Epoch:92, training loss:0.0007675886154174805, test loss:3.2531018257141113, test acc:0.712
Epoch:93, training loss:0.0018969178199768066, test loss:3.1240367889404297, test acc:0.7275
Epoch:94, training loss:0.00017076730728149414, test loss:3.293923854827881, test acc:0.7175
Epoch:95, training loss:2.5987625122070312e-05, test loss:3.2943220138549805, test acc:0.7235
Epoch:96, training loss:0.0009646415710449219, test loss:2.913499593734741, test acc:0.741
Epoch:97, training loss:8.463859558105469e-06, test loss:4.0315961837768555, test acc:0.6565
Epoch:98, training loss:0.008028507232666016, test loss:2.6604056358337402, test acc:0.739
Epoch:99, training loss:4.756450653076172e-05, test loss:3.262458562850952, test acc:0.7155
Epoch:100, training loss:0.005417823791503906, test loss:2.771040439605713, test acc:0.7425
Epoch:101, training loss:0.00040394067764282227, test loss:3.119797468185425, test acc:0.7185
Epoch:102, training loss:6.723403930664062e-05, test loss:3.1116766929626465, test acc:0.7235
Epoch:103, training loss:0.0001913309097290039, test loss:3.1764698028564453, test acc:0.722
Epoch:104, training loss:6.973743438720703e-05, test loss:3.228323221206665, test acc:0.724
Epoch:105, training loss:1.6987323760986328e-05, test loss:3.275679588317871, test acc:0.7215
Epoch:106, training loss:0.0, test loss:3.4249463081359863, test acc:0.7195
Epoch:107, training loss:5.304813385009766e-06, test loss:3.4965224266052246, test acc:0.719
Epoch:108, training loss:0.0, test loss:3.9123165607452393, test acc:0.7175
Epoch:109, training loss:1.0132789611816406e-05, test loss:2.980954647064209, test acc:0.7345
Epoch:110, training loss:0.0001747608184814453, test loss:3.0921807289123535, test acc:0.723
Epoch:111, training loss:1.71661376953125e-05, test loss:3.063652753829956, test acc:0.7355
Epoch:112, training loss:4.661083221435547e-05, test loss:3.112994909286499, test acc:0.733
Epoch:113, training loss:9.059906005859375e-06, test loss:3.1747829914093018, test acc:0.735
Epoch:114, training loss:1.811981201171875e-05, test loss:3.204885482788086, test acc:0.7365
Epoch:115, training loss:1.7344951629638672e-05, test loss:3.43381404876709, test acc:0.7295
Epoch:116, training loss:0.0013453364372253418, test loss:3.993471622467041, test acc:0.702
Epoch:117, training loss:0.007265746593475342, test loss:4.463224411010742, test acc:0.6665
Epoch:118, training loss:0.0002244710922241211, test loss:3.4212002754211426, test acc:0.7165
Epoch:119, training loss:0.00015020370483398438, test loss:3.4107255935668945, test acc:0.7205
Epoch:120, training loss:1.6689300537109375e-06, test loss:3.381601333618164, test acc:0.721
Epoch:121, training loss:4.8995018005371094e-05, test loss:3.4173989295959473, test acc:0.723
Epoch:122, training loss:0.00014507770538330078, test loss:3.5232205390930176, test acc:0.7195
Epoch:123, training loss:0.0011410117149353027, test loss:3.5521020889282227, test acc:0.7255
Epoch:124, training loss:3.4928321838378906e-05, test loss:3.1639413833618164, test acc:0.7455
Epoch:125, training loss:0.0006238222122192383, test loss:3.1254630088806152, test acc:0.74
Epoch:126, training loss:3.463029861450195e-05, test loss:3.5443146228790283, test acc:0.7105
Epoch:127, training loss:2.0325183868408203e-05, test loss:3.4570822715759277, test acc:0.722
Epoch:128, training loss:0.00023764371871948242, test loss:3.3805229663848877, test acc:0.7275
Epoch:129, training loss:1.0728836059570312e-06, test loss:3.4629688262939453, test acc:0.726
Epoch:130, training loss:1.1920928955078125e-06, test loss:3.508037567138672, test acc:0.725
Epoch:131, training loss:8.344650268554688e-07, test loss:3.601409912109375, test acc:0.725
Epoch:132, training loss:3.2007694244384766e-05, test loss:3.7014594078063965, test acc:0.7245
Epoch:133, training loss:7.68899917602539e-06, test loss:3.758458375930786, test acc:0.726
Epoch:134, training loss:1.7881393432617188e-06, test loss:3.8023343086242676, test acc:0.7295
Epoch:135, training loss:2.384185791015625e-06, test loss:3.9500913619995117, test acc:0.723
Epoch:136, training loss:6.4373016357421875e-06, test loss:3.8648529052734375, test acc:0.7335
Epoch:137, training loss:1.4662742614746094e-05, test loss:4.1945390701293945, test acc:0.7255
Epoch:138, training loss:0.0032292604446411133, test loss:3.793837547302246, test acc:0.746
Epoch:139, training loss:0.005065500736236572, test loss:2.802509069442749, test acc:0.7615
Epoch:140, training loss:0.00011706352233886719, test loss:3.60426926612854, test acc:0.7325
Epoch:141, training loss:1.8715858459472656e-05, test loss:3.577136278152466, test acc:0.733
Epoch:142, training loss:0.00038248300552368164, test loss:3.677539348602295, test acc:0.7275
Epoch:143, training loss:1.6689300537109375e-06, test loss:3.730074882507324, test acc:0.723
Epoch:144, training loss:4.00543212890625e-05, test loss:3.732588529586792, test acc:0.726
Epoch:145, training loss:1.33514404296875e-05, test loss:3.7765164375305176, test acc:0.7275
Epoch:146, training loss:1.1205673217773438e-05, test loss:3.8383007049560547, test acc:0.729
Epoch:147, training loss:2.0503997802734375e-05, test loss:3.8421902656555176, test acc:0.731
Epoch:148, training loss:7.152557373046875e-07, test loss:3.9163033962249756, test acc:0.7305
Epoch:149, training loss:2.0265579223632812e-06, test loss:3.9747140407562256, test acc:0.731
Epoch:150, training loss:1.806020736694336e-05, test loss:4.009741306304932, test acc:0.734
Epoch:151, training loss:0.0, test loss:4.122588634490967, test acc:0.7325
Epoch:152, training loss:1.7881393432617188e-06, test loss:4.273439407348633, test acc:0.7295
Epoch:153, training loss:7.152557373046875e-07, test loss:4.345433712005615, test acc:0.7315
Epoch:154, training loss:0.06957316398620605, test loss:4.42750883102417, test acc:0.7205
Epoch:155, training loss:2.0265579223632812e-06, test loss:4.066822052001953, test acc:0.73
Epoch:156, training loss:2.384185791015625e-07, test loss:3.946810245513916, test acc:0.7345
Epoch:157, training loss:0.0001335740089416504, test loss:4.130515098571777, test acc:0.7265
Epoch:158, training loss:1.0728836059570312e-06, test loss:4.147172927856445, test acc:0.7275
Epoch:159, training loss:3.504753112792969e-05, test loss:4.144064426422119, test acc:0.7275
Epoch:160, training loss:2.384185791015625e-07, test loss:4.179835319519043, test acc:0.7295
Epoch:161, training loss:0.0, test loss:4.219999313354492, test acc:0.728
Epoch:162, training loss:7.259845733642578e-05, test loss:4.397259712219238, test acc:0.716
Epoch:163, training loss:2.2649765014648438e-06, test loss:4.380931854248047, test acc:0.7195
Epoch:164, training loss:0.0, test loss:4.4756622314453125, test acc:0.7165
Epoch:165, training loss:0.0, test loss:4.519176959991455, test acc:0.7185
Epoch:166, training loss:3.5762786865234375e-07, test loss:4.529442310333252, test acc:0.7195
Epoch:167, training loss:1.0132789611816406e-06, test loss:4.601856231689453, test acc:0.7195
Epoch:168, training loss:5.161762237548828e-05, test loss:4.8251566886901855, test acc:0.7175
Epoch:169, training loss:0.0, test loss:3.871424436569214, test acc:0.738
Epoch:170, training loss:2.384185791015625e-07, test loss:5.066103935241699, test acc:0.688
Epoch:171, training loss:1.1920928955078125e-07, test loss:4.128498554229736, test acc:0.719
Epoch:172, training loss:5.364418029785156e-07, test loss:4.094317436218262, test acc:0.716
Epoch:173, training loss:1.5497207641601562e-05, test loss:4.48177433013916, test acc:0.7065
Epoch:174, training loss:2.384185791015625e-07, test loss:4.264720916748047, test acc:0.719
Epoch:175, training loss:1.1920928955078125e-07, test loss:4.291891098022461, test acc:0.7175
Epoch:176, training loss:5.364418029785156e-07, test loss:4.392663478851318, test acc:0.7165
Epoch:177, training loss:8.940696716308594e-07, test loss:4.361076831817627, test acc:0.717
Epoch:178, training loss:2.7418136596679688e-06, test loss:4.433311462402344, test acc:0.7175
Epoch:179, training loss:4.172325134277344e-07, test loss:4.41797399520874, test acc:0.7195
Epoch:180, training loss:2.384185791015625e-07, test loss:4.642093658447266, test acc:0.7205
Epoch:181, training loss:2.8908252716064453e-06, test loss:4.9641218185424805, test acc:0.6895
Epoch:182, training loss:0.0001590251922607422, test loss:4.171073913574219, test acc:0.7115
Epoch:183, training loss:6.9141387939453125e-06, test loss:4.15962028503418, test acc:0.7245
Epoch:184, training loss:0.0, test loss:4.30262565612793, test acc:0.723
Epoch:185, training loss:2.4974346160888672e-05, test loss:4.222471714019775, test acc:0.716
Epoch:186, training loss:2.568960189819336e-05, test loss:4.283426284790039, test acc:0.721
Epoch:187, training loss:1.1920928955078125e-07, test loss:4.144865989685059, test acc:0.7295
Epoch:188, training loss:4.83393669128418e-05, test loss:4.130201816558838, test acc:0.729
Epoch:189, training loss:1.1920928955078125e-06, test loss:4.129580974578857, test acc:0.7315
Epoch:190, training loss:7.88569450378418e-05, test loss:4.213004112243652, test acc:0.7325
Epoch:191, training loss:0.0, test loss:4.222304344177246, test acc:0.7315
Epoch:192, training loss:2.384185791015625e-07, test loss:4.318987846374512, test acc:0.7315
Epoch:193, training loss:1.1920928955078125e-06, test loss:4.3142991065979, test acc:0.7315
Epoch:194, training loss:0.0, test loss:4.375365257263184, test acc:0.734
Epoch:195, training loss:4.0531158447265625e-06, test loss:4.366889476776123, test acc:0.735
Epoch:196, training loss:0.0, test loss:4.407540321350098, test acc:0.7325
Epoch:197, training loss:2.1457672119140625e-06, test loss:4.446748733520508, test acc:0.7365
Epoch:198, training loss:3.159046173095703e-06, test loss:4.524875640869141, test acc:0.712
Epoch:199, training loss:3.5762786865234375e-07, test loss:4.936115264892578, test acc:0.7165
Epoch:200, training loss:0.08679148554801941, test loss:3.925751209259033, test acc:0.751
